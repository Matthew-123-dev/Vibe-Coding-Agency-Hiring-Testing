{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920afe5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Question 1.1**\n",
    "I will walk through this with four essential steps.\n",
    "\n",
    "**Step 1:** The Intake and Context Agent\n",
    "- **Input:** The code changes (PR Diff) and the project's Readme.\n",
    "- **Output:** A summary of what the code is trying to do (e.g. \"This updates the page animations\").\n",
    "- **Success Criteria:** Correctly identifies which part of the system. \n",
    "- **Failure Handling:** If the code is too messy to read, it notifies the users so that a professional can run a check manually.\n",
    "\n",
    "**Step 2:** The Security & Quality Audit\n",
    "- **Input:** The code changes + a list of \"Security Rules\" (OWASP).\n",
    "- **Output:** A checklist of passes/fails (e.g., \"Pass: No hardcoded passwords. Fail: Missing error handling on line 42\").\n",
    "- **Success Criteria:** Catches critical bugs before a human even sees it.\n",
    "- **Failure Handling:** If the AI is unsure, it marks the line as \"Needs Senior Review.\"\n",
    "\n",
    "**Step 3:** Automated Deployment Orchestrator\n",
    "- **Input:** The \"Passed\" audit report from the security and quality audit.\n",
    "- **Output:** Commands to push the code to \"Staging\" (the testing site) or \"Production\" (the live site).\n",
    "- **Success Criteria:** Code reaches the server without crashing.\n",
    "- **Failure Handling:** If the server doesn't respond, it stops the deployment immediately.\n",
    "\n",
    "**Step 4:** The Health Monitor (Rollback Agent)\n",
    "- **Input:** Server metrics (like \"Is the site slow?\" or \"Are there errors?\").\n",
    "- **Output:** A \"Keep Live\" or \"Rollback\" command.\n",
    "- **Success Criteria:** If the site crashes, it automatically puts the old version back in under 60 seconds.\n",
    "- **Failure Handling:** Alerts the engineering team on Slack immediately if a rollback fails.\n",
    "\n",
    "\n",
    "**Question 1.2**\n",
    "- **Blocking:** Step 3 must wait for step 2. The security of the code has to be confirmed before it gets deployed.\n",
    "- **Parallel:** Security and coding style can be checked at the same time.\n",
    "-**Critical Decision Point:** Between Step 2 and 3. This is the \"Gate.\" If the AI gives a low confidence score, a human must click \"Approve.\"\n",
    "\n",
    "\n",
    "**Question 1.3**\n",
    "The data that needs to be passed across each stage are as follows:\n",
    "- **From 1 to 2:** The context (i.e What the code is for.).\n",
    "- **From 2 to 3:** The security token, proof that the code is safe.\n",
    "- **from 3 to 4:** The \"Version ID\" (So the monitor knows which version to roll back to if things break)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831a4da",
   "metadata": {},
   "source": [
    "#### **Question 2.1: Designing the Prompts**\n",
    "\n",
    "**Prompt 1: The Intake and Context Agent**\n",
    "**System Persona:** You are a Software Engineer and Code Reviewer, and your goal is to identify code changes in the project. You are precise and are to carry out your task with scrutiny.\n",
    "\n",
    "**Input:** \n",
    "- `Project_context` (i.e python/Flask app)\n",
    "- `Code_Diff` (Actual code changes)\n",
    "\n",
    "**Expected Output format:** Text- Straightforward and detailed description of changes made (e.g \"This updates the login button.\")\n",
    "\n",
    "**Good vs. Bad:** \n",
    "- Good- \"The search algorithm in `algo.py` was changed from linear to binary search. The button style in `styles.css` was updated, the button colour was changed from grey to black.\"\n",
    "- Bad- \"The files `algo.py` and `styles.css` where changed.\"\n",
    "\n",
    "**Error Handling:** \n",
    "\n",
    "\n",
    "\n",
    "**Prompt 2: The Security & Quality Audit Agent**\n",
    "**System Persona:** You are an expert Full-Stack Security Engineer and Code Reviewer. Your goal is to identify vulnerabilities and logic flaws in code diffs. You are pedantic, precise, and prioritize security over features.\n",
    "\n",
    "**Input:** \n",
    "- `Project_context` (i.e python/Flask app)\n",
    "- `Code_Diff` (Actual code changes)\n",
    "- `Security_Standards` (e.g. OWASP)\n",
    "\n",
    "**Expected Output format:** ```json { \"risk_level\": \"High/Med/Low\", \"issues\": [{\"line\": 24, \"type\": \"SQL Injection\", \"fix\": \"Use parameterized queries\"}], \"confidence_score\": 0.95 }\n",
    "\n",
    "**Good vs. Bad:** \n",
    "- Good- \"Specific line numbers and actionable fixes.\n",
    "- Bad- \"The code looks okay\" or general advice without context.\n",
    "\n",
    "**Error Handling:** If the code diff is incomplete or unparseable, return an error code `ERR_INVALID_DIFF` and request the full file.\n",
    "\n",
    "\n",
    "#### **Question 2.2:Handling Challenging Scenarios**\n",
    "\n",
    "- **Obscure Libraries:** I would use a \"RAG\" (Retrieval-Augmented Generation) approach. The prompt would instruct the AI to first search the provided library documentation (attached as context) before reviewing the code.\n",
    "\n",
    "- **Security Reviews:** I would use \"Chain of Thought\" prompting. I'll ask the AI to: \"1. List all entry points for user input. 2. Trace how that input reaches the database. 3. Identify where sanitization is missing.\"\n",
    "\n",
    "- **Performance Analysis:** I would feed the AI the `EXPLAIN ANALYZE` output from the database. This gives the AI the actual execution plan, not just the code, to see where the \"bottleneck\" is.\n",
    "\n",
    "- **Legacy Code:** I would provide the AI with a \"Legacy Context Map.\" The prompt would say: \"Do not suggest modern Python 3.12 features; this environment is restricted to Python 2.7. Focus only on logic fixes.\"\n",
    "\n",
    "\n",
    "\n",
    "#### **Question 2.3: Ensuring Consistency**\n",
    "\n",
    "To ensure the AI doesn't give different answers every time, I would:\n",
    "1. **Lower the \"Temperature\":** Set the AI model temperature to 0.0 or 0.1 to make it predictable and \"boring\" rather than creative.\n",
    "2. **Give Examples:** Include 3-5 examples of \"Perfect Reviews\" within the prompt so the AI has a template to copy.\n",
    "3. **Validate Output:** Use a separate script (or Pydantic in Python) to verify that the AI's response is valid JSON before it moves to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf764ae",
   "metadata": {},
   "source": [
    "#### **Question 3.1: Making the System Reusable**\n",
    "\n",
    "To make the system work across 50+ microservices and diverse teams, I would implement a \"Configuration-as-Code\" model:\n",
    "\n",
    "- **Centralized Policy Engine:** Instead of hardcoding rules, use a global config file (YAML/JSON). Each team can \"subscribe\" to certain rules (e.g., \"Financial Team\" turns on strict HIPAA compliance checks, while \"Internal Tools\" uses standard checks).\n",
    "\n",
    "- **Adapter Pattern for CI/CD:** Build \"adapters\" for different platforms. The core AI logic stays the same, but the \"handshake\" changes whether the team uses GitHub Actions, GitLab, or Jenkins.\n",
    "\n",
    "- **Prompt Templating:** Use a library like LangChain to manage prompts. The system detects the programming language (Python, Java, Go) and automatically injects the correct language-specific security rules into the AI's \"persona.\"\n",
    "\n",
    "- **Secret Management:** Integrate with tools like HashiCorp Vault or AWS Secrets Manager to ensure the AI system never handles raw credentials, keeping it compliant with industry standards (SOC2/ISO27001).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Question 3.2: Continuous Improvement**\n",
    "\n",
    "The system should get \"smarter\" every week by closing the feedback loop:\n",
    "\n",
    "- **The \"Human-in-the-Loop\" Signal:** When a developer clicks \"Approve\" on a PR that the AI flagged as \"High Risk,\" the system logs this as a False Positive. This data is fed back into the prompt engineering team to refine the AI’s instructions.\n",
    "\n",
    "- **Production Correlation:** If a deployment passes the AI review but causes a production crash, the system performs an Automated Post-Mortem. It analyzes the \"missed\" bug and adds a new \"Guardrail\" to the AI's checklist to prevent that specific bug from ever slipping through again.\n",
    "\n",
    "- **A/B Testing Prompts:** eriodically run two versions of the AI reviewer (e.g., Prompt A vs. Prompt B) and measure which one results in fewer developer \"dismissals\" and faster approval times.\n",
    "\n",
    "- **Developer Sentiment:** Include a simple \"Was this review helpful?\" button. If a specific agent gets low ratings, it is automatically flagged for retraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6c8b7",
   "metadata": {},
   "source": [
    "#### **Question 4.1: Prioritization & 6-Month Roadmap**\n",
    "\n",
    "My implementation strategy follows a \"crawl-walk-run\" approach to build trust with the engineering team while delivering immediate value.\n",
    "\n",
    "* **Months 1-2: MVP (Minimum Viable Product)**\n",
    "* **Focus:** Automating the \"Security & Quality Audit\" for a single high-velocity Python team.\n",
    "* **Goal:** Reduce manual review time for style and common security flaws.\n",
    "* **Success Metrics:** Average PR cycle time reduced by 30%; 0 critical security leaks in the pilot service.\n",
    "\n",
    "\n",
    "* **Months 3-4: Pilot Expansion & Automation**\n",
    "* **Focus:** Integrate the AI Agent with GitHub Actions and implement the \"Health Monitor\" for automated rollbacks in the Staging environment.\n",
    "* **Goal:** Establish the \"automated gate\" where AI can block a deployment if security or health checks fail.\n",
    "* **Success Metrics:** 90% accuracy in automated rollback triggers; 50% reduction in total review time across 5 teams.\n",
    "\n",
    "\n",
    "* **Months 5-6: Full Scale & Platformization**\n",
    "* **Focus:** Scale to 50+ microservices and add support for Java and Go. Launch a \"Self-Service Dashboard\" for teams to customize their AI review rules.\n",
    "* **Goal:** Standardize the deployment lifecycle across the entire organization.\n",
    "* **Success Metrics:** < 4-hour average PR review time company-wide; > 80% developer satisfaction rate.\n",
    "\n",
    "\n",
    "\n",
    "#### **Question 4.2: Risk Mitigation**\n",
    "\n",
    "* **AI Logic Errors:** I will implement a \"Confidence Threshold.\" If the AI’s confidence score is below 85%, it is forced to flag a human senior reviewer rather than making a solo decision.\n",
    "* **System Downtime:** The architecture will use a **\"Fail-Open\"** mechanism. If the AI service is unreachable, the pipeline reverts to manual human approval so that critical hotfixes are never blocked by the tool itself.\n",
    "* **Team Resistance:** To prevent friction, we will launch a \"Transparency Dashboard\" showing exactly why the AI makes certain decisions, and provide a \"One-Click Dispute\" button for developers to challenge AI feedback.\n",
    "* **Compliance/Audit:** Every decision, prompt, and output will be logged in an immutable audit trail to satisfy SOC2 and industry-specific compliance requirements.\n",
    "\n",
    "### **Question 4.3: Tool Selection**\n",
    "\n",
    "To ensure reliability and speed, I will integrate the system with the following industry-standard tools:\n",
    "\n",
    "| Category | Tool | Purpose |\n",
    "| --- | --- | --- |\n",
    "| **Code Review** | **GitHub / GitLab** | Primary interface for code hosting and PR comments via Webhooks. |\n",
    "| **CI/CD** | **GitHub Actions** | Orchestrates the movement of code between Dev, Staging, and Prod. |\n",
    "| **Monitoring** | **Datadog / Prometheus** | Provides the real-time telemetry (error rates, latency) for rollback decisions. |\n",
    "| **Security** | **Snyk / SonarQube** | Used as the primary \"engine\" for scanning, with AI used to interpret and suggest fixes for the findings. |\n",
    "| **Communication** | **Slack** | Real-time alerts for the engineering team regarding deployment status or failures. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
